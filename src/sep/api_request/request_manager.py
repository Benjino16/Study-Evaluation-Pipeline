"""
Module to process text or PDF files using various AI models like Gemini, GPT, or Ollama.
It selects prompts dynamically and handles batch processing with optional delay and error handling.
"""

from .gemini_pipeline import process_file_with_gemini
from .gpt_pipeline import process_pdf_with_openai
from .gpt_text_pipeline import process_text_with_openai
from .ollama_pipeline import process_text_with_ollama
from.test_pipeline import process_test_pipeline
import logging

logging.basicConfig(level=logging.INFO)

def run_request(prompt: str, file_path: str, model: str, process_all: bool, pdf_reader: bool, delay: int, temperature: float) -> str:
    """
    Runs a request with the given parameters and returns the model output along with the used prompt.

    Returns:
        str: Raw model output and prompt used.
    """
    if not process_all:
        raise ValueError("The --process_all argument is not supported anymore")


    return run_prompt(prompt, file_path, model, pdf_reader, temperature)


def run_prompt(prompt: str, file_path: str, model: str, pdf_reader: bool, temperature: float):
    """
    Processes a file or text input with the selected model and prompt.

    Returns:
        str: The output generated by the model.
    """

    if model.lower().startswith('gemini'):
        if pdf_reader:
            # Gemini does not support PDF reader mode
            raise ValueError("A request to gemini is not possible with a PDF reader. Do not use --pdf_reader in the arguments.")
        else:
            last_output = process_file_with_gemini(prompt, file_path, model, temperature)

    elif model.lower().startswith('gpt') or model.lower().startswith('deepseek-chat'):
        if pdf_reader:
            # Process text input with GPT
            last_output = process_text_with_openai(prompt, file_path, model, temperature)
        else:
            # Process PDF input with GPT
            last_output = process_pdf_with_openai(prompt, file_path, model, temperature)

    elif model.lower().startswith('o1'):
        if pdf_reader:
            # Process text input with OpenAI
            last_output = process_text_with_openai(prompt, file_path, model, temperature)
        else:
            # O1 models require PDF reader mode
            raise ValueError("A request to an o1 model is not possible without a PDF reader. Use --pdf_reader in the arguments.")

    elif model.lower().startswith('deepseek'):
        if pdf_reader:
            # Process text input with Ollama
            last_output = process_text_with_ollama(prompt, file_path, model, temperature)
        else:
            # Ollama models require PDF reader mode
            raise ValueError("A request to an ollama model is not possible without a PDF reader. Use --pdf_reader in the arguments.")
        
    elif model.lower().startswith('test'):
        last_output = process_test_pipeline(prompt, file_path, model, temperature)
    else:
        raise ValueError(f"Unsupported model: {model}")


    return last_output
